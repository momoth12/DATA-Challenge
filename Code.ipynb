{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lelia\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\lelia\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clients = pd.read_csv('../data/Clients.csv', delimiter=';', encoding='latin1')\n",
    "commandes = pd.read_csv('../data/Commandes.csv', delimiter=';', encoding='latin1')\n",
    "envois_chunks = pd.read_csv('../data/Envois.csv', delimiter=';', encoding='latin1', chunksize=500000)\n",
    "envois = envois_chunks.get_chunk()\n",
    "produits = pd.read_csv('../data/Produits.csv', delimiter=';', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups\n",
    "bins = [25, 30, 35, 40, 45, 50, 55, 60, 65, 100]\n",
    "labels = ['25-30', '30-35', '35-40', '40-45', '45-50', '50-55', '55-60', '60-65', '65+']\n",
    "clients['Age Group'] = pd.cut(clients['Age'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge clients & commandes\n",
    "merged_clients_comm = pd.merge(clients, commandes, on='id_client', how='inner')\n",
    "\n",
    "# merge commandes & produits\n",
    "merged_comm_prod = pd.merge(commandes, produits, on='id_produit', how='inner')\n",
    "\n",
    "# merge all three tables\n",
    "merged_clients_comm_prod = pd.merge(merged_clients_comm, produits, on='id_produit', how='inner')\n",
    "merged_clients_comm_prod['Montant_Produit'] = merged_clients_comm_prod['Montant_Produit'].str.replace(',', '.').astype(float)\n",
    "merged_clients_comm_prod['Montant_Remise'] = merged_clients_comm_prod['Montant_Remise'].str.replace(',', '.').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_client</th>\n",
       "      <th>Host</th>\n",
       "      <th>Pays</th>\n",
       "      <th>Age</th>\n",
       "      <th>Anciennete_Compte</th>\n",
       "      <th>Member_Programme_de_fid</th>\n",
       "      <th>A_ete_Membre_Programme_de_fid</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Web_Count</th>\n",
       "      <th>Boutique_Count</th>\n",
       "      <th>Count_Orders_year</th>\n",
       "      <th>Count_Orders_3months</th>\n",
       "      <th>Count_Orders_6months</th>\n",
       "      <th>Total_Amount_Spent</th>\n",
       "      <th>Count_Orders_Launch_Year</th>\n",
       "      <th>Orders_After</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id_client, Host, Pays, Age, Anciennete_Compte, Member_Programme_de_fid, A_ete_Membre_Programme_de_fid, Age Group, Web_Count, Boutique_Count, Count_Orders_year, Count_Orders_3months, Count_Orders_6months, Total_Amount_Spent, Count_Orders_Launch_Year, Orders_After]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = merged_clients_comm_prod.copy()\n",
    "\n",
    "# Count of orders by Boutique and Web per client\n",
    "web_counts = data_df[data_df['Canal_Commande'] == 'Web'].groupby('id_client').size().reset_index(name='Web_Count')\n",
    "boutique_counts = data_df[data_df['Canal_Commande'] == 'Boutique'].groupby('id_client').size().reset_index(name='Boutique_Count')\n",
    "\n",
    "result = pd.merge(web_counts, boutique_counts, on='id_client', how='outer').fillna(0).astype(int)\n",
    "\n",
    "# nombre total de commandes dans les 12, 6 et 3 derniers mois\n",
    "count_orders_year = data_df[data_df['dCommande'] > '2022-07-01'].groupby('id_client').size().reset_index(name='Count_Orders_year')\n",
    "count_orders_6 = data_df[data_df['dCommande'] > '2023-01-01'].groupby('id_client').size().reset_index(name='Count_Orders_6months')\n",
    "count_orders_3 = data_df[data_df['dCommande'] > '2023-04-01'].groupby('id_client').size().reset_index(name='Count_Orders_3months')\n",
    "\n",
    "# Total amount spent per id_client\n",
    "amount_spent = data_df.groupby('id_client')['Montant_Produit'].sum().reset_index(name='Total_Amount_Spent')\n",
    "\n",
    "# Count of products bought in their launch year\n",
    "data_df['Year'] = str(data_df['dCommande'])[0:3]\n",
    "count_orders_launch_year = data_df[data_df['Year'] == data_df['Annee_lancement_produit']].groupby('id_client').size().reset_index(name='Count_Orders_Launch_Year')\n",
    "\n",
    "result = pd.merge(result, count_orders_year, on='id_client', how='inner').astype(int)\n",
    "result = pd.merge(result, count_orders_3, on='id_client', how='inner').astype(int)\n",
    "result = pd.merge(result, count_orders_6, on='id_client', how='inner').astype(int)\n",
    "result = pd.merge(result, amount_spent, on='id_client', how='inner').astype(float)\n",
    "result = pd.merge(result, count_orders_launch_year, on='id_client', how='inner').astype(int)\n",
    "\n",
    "# Count of orders placed after a limit date\n",
    "limit_date = '2023-01-01'\n",
    "\n",
    "orders_after = data_df[data_df['dCommande'] >= limit_date]\n",
    "order_counts = orders_after.groupby('id_client').size().reset_index(name='Orders_After')\n",
    "\n",
    "result = pd.merge(result, order_counts, on='id_client', how='inner')\n",
    "\n",
    "# Final merge \n",
    "dataset_af = pd.merge(clients, result, on='id_client', how='inner')\n",
    "\n",
    "dataset_af = dataset_af.drop(['dPremierEnvoi', 'dPremiereCommande'], axis=1, errors='ignore')\n",
    "\n",
    "dataset_af = pd.get_dummies(dataset_af, columns=['Segmentation','Civilite', 'ZoneGeographique'])\n",
    "dataset_af = dataset_af.dropna()\n",
    "\n",
    "dataset_af.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_client', 'Host', 'Pays', 'Age', 'Anciennete_Compte',\n",
      "       'dPremierEnvoi', 'dPremiereCommande', 'Member_Programme_de_fid',\n",
      "       'A_ete_Membre_Programme_de_fid', 'Age Group', 'Web_Count',\n",
      "       'Boutique_Count', 'Count_Orders_year', 'Count_Orders_3months',\n",
      "       'Count_Orders_6months', 'Total_Amount_Spent',\n",
      "       'Count_Orders_Launch_Year', 'Orders_After',\n",
      "       'Segmentation_Actifs 12-36 mois multi-acheteurs',\n",
      "       'Segmentation_Nouveaux - 6 mois', 'Segmentation_Ractivs - 6 mois'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Filter the data based on the date of the commandes\n",
    "train_date = '2023-04-01'\n",
    "data_bf = merged_clients_comm_prod.copy()[merged_clients_comm_prod['dCommande'] <= train_date]\n",
    "\n",
    "# Count of orders by Boutique and Web per client\n",
    "web_counts = data_bf[data_bf['Canal_Commande'] == 'Web'].groupby('id_client').size().reset_index(name='Web_Count')\n",
    "boutique_counts = data_bf[data_bf['Canal_Commande'] == 'Boutique'].groupby('id_client').size().reset_index(name='Boutique_Count')\n",
    "\n",
    "result = pd.merge(web_counts, boutique_counts, on='id_client', how='outer').fillna(0).astype(int)\n",
    "\n",
    "# nombre total de commandes dans les 12, 6 et 3 derniers mois\n",
    "count_orders_year = data_bf[data_bf['dCommande'] > '2022-04-01'].groupby('id_client').size().reset_index(name='Count_Orders_year')\n",
    "count_orders_6 = data_bf[data_bf['dCommande'] > '2022-10-01'].groupby('id_client').size().reset_index(name='Count_Orders_6months')\n",
    "count_orders_3 = data_bf[data_bf['dCommande'] > '2023-01-01'].groupby('id_client').size().reset_index(name='Count_Orders_3months')\n",
    "\n",
    "# Total amount spent per id_client\n",
    "amount_spent = data_bf.groupby('id_client')['Montant_Produit'].sum().reset_index(name='Total_Amount_Spent')\n",
    "\n",
    "# Count of products bought in their launch year\n",
    "data_bf['Year'] = str(data_bf['dCommande'])[0:3]\n",
    "count_orders_launch_year = data_bf[data_bf['Year'] == data_bf['Annee_lancement_produit']].groupby('id_client').size().reset_index(name='Count_Orders_Launch_Year')\n",
    "\n",
    "result = pd.merge(result, count_orders_year, on='id_client', how='inner').astype(int)\n",
    "result = pd.merge(result, count_orders_3, on='id_client', how='inner').astype(int)\n",
    "result = pd.merge(result, count_orders_6, on='id_client', how='inner').astype(int)\n",
    "result = pd.merge(result, amount_spent, on='id_client', how='inner').astype(float)\n",
    "result = pd.merge(result, count_orders_launch_year, on='id_client', how='inner').astype(int)\n",
    "\n",
    "# Count of orders placed after a limit date\n",
    "limit_date = '2023-01-01'\n",
    "\n",
    "orders_after = data_bf[data_bf['dCommande'] >= limit_date]\n",
    "order_counts = orders_after.groupby('id_client').size().reset_index(name='Orders_After')\n",
    "\n",
    "result = pd.merge(result, order_counts, on='id_client', how='inner')\n",
    "\n",
    "# Final merge \n",
    "dataset_bf = pd.merge(clients, result, on='id_client', how='inner')\n",
    "\n",
    "dataset_af = dataset_af.drop(['dPremierEnvoi', 'dPremiereCommande'], axis=1, errors='ignore')\n",
    "\n",
    "dataset_bf = dataset_bf.dropna()\n",
    "\n",
    "dataset_bf = pd.get_dummies(dataset_bf, columns=['Segmentation','Civilite', 'ZoneGeographique'])\n",
    "\n",
    "dataset_bf['Segmentation_Actifs 12-36 mois multi-acheteurs'] = 0\n",
    "dataset_bf['Segmentation_Nouveaux - 6 mois'] = 0\n",
    "dataset_bf['Segmentation_Ractivs - 6 mois'] = 0\n",
    "\n",
    "print(dataset_bf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Anciennete_Compte', 'dPremierEnvoi', 'dPremiereCommande',\n",
      "       'Member_Programme_de_fid', 'A_ete_Membre_Programme_de_fid', 'Web_Count',\n",
      "       'Boutique_Count', 'Count_Orders_year', 'Count_Orders_3months',\n",
      "       'Count_Orders_6months', 'Total_Amount_Spent',\n",
      "       'Count_Orders_Launch_Year', 'Orders_After',\n",
      "       'Segmentation_Actifs 12-36 mois multi-acheteurs',\n",
      "       'Segmentation_Nouveaux - 6 mois', 'Segmentation_Ractivs - 6 mois'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lelia\\Documents\\0_X3A\\MIE512C - Data Marketing et expérience client\\DATA-Challenge\\DATA-Challenge\\Code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/MIE512C%20-%20Data%20Marketing%20et%20exp%C3%A9rience%20client/DATA-Challenge/DATA-Challenge/Code.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/MIE512C%20-%20Data%20Marketing%20et%20exp%C3%A9rience%20client/DATA-Challenge/DATA-Challenge/Code.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m numerical_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mWeb_Count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBoutique_Count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCount_Orders_year\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCount_Orders_3months\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCount_Orders_6months\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTotal_Amount_Spent\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/MIE512C%20-%20Data%20Marketing%20et%20exp%C3%A9rience%20client/DATA-Challenge/DATA-Challenge/Code.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mCount_Orders_Launch_Year\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/MIE512C%20-%20Data%20Marketing%20et%20exp%C3%A9rience%20client/DATA-Challenge/DATA-Challenge/Code.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m dataset_bf[numerical_cols] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(dataset_bf[numerical_cols])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/MIE512C%20-%20Data%20Marketing%20et%20exp%C3%A9rience%20client/DATA-Challenge/DATA-Challenge/Code.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/MIE512C%20-%20Data%20Marketing%20et%20exp%C3%A9rience%20client/DATA-Challenge/DATA-Challenge/Code.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_data, test_data \u001b[39m=\u001b[39m train_test_split(dataset_bf, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \n\u001b[0;32m    843\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 873\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[0;32m    875\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    876\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    877\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    878\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    882\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    968\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 969\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    973\u001b[0m         )\n\u001b[0;32m    975\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    976\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess the data\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['id_client', 'Host', 'Age Group', 'Pays']\n",
    "dataset_bf = dataset_bf.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "print(dataset_bf.columns)\n",
    "\n",
    "# Normalize numerical variables\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Web_Count', 'Boutique_Count', 'Count_Orders_year', 'Count_Orders_3months', 'Count_Orders_6months', 'Total_Amount_Spent', \n",
    "                  'Count_Orders_Launch_Year']\n",
    "dataset_bf[numerical_cols] = scaler.fit_transform(dataset_bf[numerical_cols])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, test_data = train_test_split(dataset_bf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Build the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 3: Train the model\n",
    "train_inputs = train_data.drop('Orders_After', axis=1)\n",
    "train_labels = train_data['Orders_After']\n",
    "test_inputs = test_data.drop('Orders_After', axis=1)\n",
    "test_labels = test_data['Orders_After']\n",
    "\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# Step 4: Test the model\n",
    "\n",
    "# Preprocess the validation data \n",
    "test_data = test_data.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "test_data[numerical_cols] = scaler.fit_transform(test_data[numerical_cols])\n",
    "\n",
    "# Use the trained model to make predictions on the validation data\n",
    "predictions = model.predict(test_inputs)\n",
    "\n",
    "# Get the actual labels from the validation dataset\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Step 1: Preprocess the data (same as before)\n",
    "\n",
    "# Step 2: Build the model\n",
    "model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Step 3: Train the model (same as before)\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# Step 4: Test the model on 'dataset' (same as before)\n",
    "predictions = model.predict(test_inputs)\n",
    "\n",
    "# Calculate the accuracy of the model (same as before)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Step 1: Preprocess the data (same as before)\n",
    "\n",
    "# Step 2: Build the model\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Step 3: Train the model (same as before)\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# Step 4: Test the model on 'dataset' (same as before)\n",
    "predictions = model.predict(test_inputs)\n",
    "\n",
    "# Calculate the accuracy of the model (same as before)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Step 1: Preprocess the data (same as before)\n",
    "input_size = train_inputs.shape[1]  # Using shape[1] to get the number of columns\n",
    "\n",
    "# Convert the input data to tensors\n",
    "train_inputs_tensor = torch.tensor(train_inputs.values, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32).view(-1, 1)  # Reshape the labels\n",
    "test_inputs_tensor = torch.tensor(test_inputs.values, dtype=torch.float32)\n",
    "\n",
    "# Step 2: Build the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)  # Removed Sigmoid for regression\n",
    ")\n",
    "\n",
    "# Step 3: Train the model (same as before)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in zip(train_inputs_tensor, train_labels_tensor):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} loss: {running_loss/len(train_inputs_tensor)}\")\n",
    "\n",
    "# Step 4: Test the model on 'val_inputs_tensor'\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_inputs_tensor)\n",
    "\n",
    "# Convert predictions and labels to numpy arrays for easier evaluation\n",
    "predictions_np = predictions.numpy()\n",
    "\n",
    "# Evaluate your regression model using appropriate metrics (e.g., Mean Absolute Error, Mean Squared Error)\n",
    "mae = mean_absolute_error(test_labels, predictions_np)\n",
    "mse = mean_squared_error(test_labels, predictions_np)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the data\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['id_client', 'Host', 'Civilite', 'Age Group', 'ZoneGeographique', 'Pays', 'dPremierEnvoi', 'dPremiereCommande']\n",
    "dataset_af = dataset_af.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "# Normalize numerical variables\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Web_Count', 'Boutique_Count', 'Count_Orders_year', 'Count_Orders_3months', 'Count_Orders_6months', 'Total_Amount_Spent', \n",
    "                  'Count_Orders_Launch_Year']\n",
    "dataset_af[numerical_cols] = scaler.fit_transform(dataset_af[numerical_cols])\n",
    "\n",
    "# Split in inputs and labels sets\n",
    "val_inputs = dataset_af.drop('Orders_After', axis=1)\n",
    "val_labels = dataset_af['Orders_After']\n",
    "\n",
    "input_size = val_inputs.shape[1]  # Using shape[1] to get the number of columns\n",
    "\n",
    "# Convert the input data to a tensor\n",
    "val_inputs_tensor = torch.tensor(val_inputs.values, dtype=torch.float32)\n",
    "\n",
    "# Step 2: Use the trained DL model to make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(val_inputs_tensor)\n",
    "\n",
    "# Step 3: Convert the predictions to the desired format\n",
    "predictions_np = predictions.numpy()\n",
    "\n",
    "mae = mean_absolute_error(val_labels, predictions_np)\n",
    "mse = mean_squared_error(val_labels, predictions_np)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
